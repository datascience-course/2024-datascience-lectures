{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Data Science \n",
    "# Lecture 9: Linear Regression 2\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we'll discuss:\n",
    "* overfitting, model generalizability, and the bias-variance tradeoff\n",
    "* cross validation \n",
    "* using categorical variables for regression  \n",
    "\n",
    "Recommended reading:\n",
    "* G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning, Ch. 3 [digitial version available here](https://d1wqtxts1xzle7.cloudfront.net/60707896/An_Introduction_to_Statistical_Learning_with_Applications_in_R-Springer_201320190925-63943-2cqzhk-libre.pdf?1569480857=&response-content-disposition=inline%3B+filename%3DAn_Introduction_to_Statistical_Learning.pdf&Expires=1675807025&Signature=EpiMSZAPOQ6yVW2yAZzLWCYz0MUeqp04yzupocLwB5CLHj5ugPf90tXrC6unkVv96az1rCxOeNnNb0bJl5KTCRUJHt7vXtu8pSC7H5vUO7NFOKf7QAmq4Z8vvvpkhN29uWD~fidv3lTH~Glg4-qdaGKf~qNg0J1cqnJ-ibwZCWjTuReWC4DMsi~00dA76CL4woM1a3uMUYsiCQF9~qchsI~t9hUFmvBByql3eFhGgxU2IJBCMJ~sUk~fnhmK-aGITL1ATOTVIa4rT1ucm7lh64ROdsu5RhEv5VpxotU~JjYbwdAvqiNB~QjAiIRELzqYvMOKiccn8uL0POR5DAdPqQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review from Lecture 8(Linear Regression 1)\n",
    "\n",
    "### Simple Linear Regression (SLR)\n",
    "\n",
    "**Data**: We have $n$ samples $(x, y)_i$, $i=1,\\ldots n$. \n",
    "\n",
    "**Model**: $y \\sim \\beta_0 + \\beta_1 x$ \n",
    "\n",
    "**Goal**: Find the best values of $\\beta_0$ and $\\beta_1$, denoted $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$, so that the prediction $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x$ \"best fits\" the data. \n",
    "\n",
    "<img src=\"438px-Linear_regression.png\" width=\"40%\" alt=\"https://en.wikipedia.org/wiki/Linear_regression\">\n",
    "\n",
    "**Theorem.** \n",
    "The parameters that minimize the \"residual sum of squares (RSS)\", \n",
    "$RSS = \\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2$, \n",
    "are: \n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) }{\\sum_{i=1}^n (x_i - \\overline{x})^2}\n",
    "\\qquad \\textrm{and} \\qquad\n",
    "\\hat{\\beta}_0 = \\overline{y} -  \\hat{\\beta}_1 \\overline{x}. \n",
    "$$\n",
    "where $\\overline{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$ and $\\overline{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$. \n",
    "\n",
    "\n",
    "### Multilinear regression \n",
    "\n",
    "**Data**: We have $n$ samples of the form $\\big(x_1, x_2 , \\ldots, x_m , y \\big)_i$, $i=1,\\ldots n$. \n",
    "\n",
    "**Model**: $y \\sim \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_m x_m $ \n",
    "\n",
    "### Nonlinear relationships  \n",
    "\n",
    "**Data**: We have $n$ samples $\\big(x_1, x_2 , \\ldots, x_m , y \\big)_i$, $i=1,\\ldots n$. \n",
    "\n",
    "**Model**: $y \\sim \\beta_0 + \\beta_1 f_1(x_1,x_2,\\ldots,x_m) + \\cdots +  \\beta_k f_k(x_1,x_2,\\ldots,x_m)$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression with python\n",
    "\n",
    "There are several different python packages that do regression:\n",
    "+ [statsmodels](http://statsmodels.sourceforge.net/)\n",
    "+ [scikit-learn](http://scikit-learn.org/)\n",
    "+ [SciPy](http://www.scipy.org/)\n",
    "+ ... \n",
    "\n",
    "Note statsmodels approaches regression from a statistics viewpoint, while scikit-learn approaches it from a machine learning viewpoint. I'll say more about this today. \n",
    "\n",
    "SciPy has some regression tools, but compared to these other two packages, they are relatively limited. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm     #Last lecture: used statsmodels.formula.api.ols() for OLS\n",
    "from sklearn import linear_model         #Last lecture: used sklearn.linear_model.LinearRegression() for OLS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "plt.rcParams['figure.figsize'] = (7.5, 4.5)\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advertisement dataset\n",
    "Consider the 'Advertising' dataset from\n",
    "[here](http://www-bcf.usc.edu/~gareth/ISL/data.html).\n",
    "\n",
    "\n",
    "For 200 different ‘markets’ (think different cities), this dataset consists of the number of sales of a particular product as well as the advertising budget for three different media: TV, radio, and newspaper. \n",
    "\n",
    "Last time, after trying a variety of linear models, we discovered the following one, which includes a nonlinear relationship between the TV budget and Radio budget:\n",
    "$$\n",
    "\\text{Sales} = \\beta_0 + \\beta_1 * \\text{TV_budget} + \\beta_2*\\text{Radio_budget} + \\beta_3 * \\text{TV_budget} *\\text{Radio_budget}. \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "advert = pd.read_csv('Advertising.csv',index_col=0) #load data\n",
    "\n",
    "ad_NL = sm.ols(formula=\"Sales ~ TV + Radio + TV*Radio\", data=advert).fit()\n",
    "ad_NL.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This model is really excellent: \n",
    "- $R^2 = 97\\%$ of the variability in the data is accounted for by the model. \n",
    "- The $p$-value for the F-statistic is very small. \n",
    "- The $p$-values for the individual coefficients are small. \n",
    "\n",
    "Interpretation: \n",
    "- In a particular market, if I spend an additional $1k on TV advertising, what do I expect sales to do? \n",
    "- Should I spend additional money on TV or Radio advertising? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Initialize figure and axes for 3d plot:\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d') \n",
    "\n",
    "#Make 3d scatter plot of the data:\n",
    "ax.scatter(xs=advert['TV'], ys=advert['Radio'], zs=advert['Sales']) \n",
    "\n",
    "#Make a surface plot of the PREDICTED response variable\n",
    "x = np.linspace(advert['TV'].min(), advert['TV'].max(), 100)\n",
    "y = np.linspace(advert['Radio'].min(), advert['Radio'].max(), 100)\n",
    "X,Y = np.meshgrid(x,y) #Create grid of values for explanatory variables\n",
    "par = dict(ad_NL.params) #Make a dictionary out of model coefficients\n",
    "print(par)\n",
    "Z = par[\"Intercept\"] + par[\"TV\"]*X + par[\"Radio\"]*Y + par[\"TV:Radio\"]*X*Y\n",
    "surf = ax.plot_surface(X, Y, Z,cmap=cm.Greys, alpha=0.2) #alpha<1 makes surface plot transparent\n",
    "\n",
    "# Rotate and label axes:\n",
    "ax.view_init(25,-71)\n",
    "ax.set_xlabel('TV budget')\n",
    "ax.set_ylabel('Radio budget')\n",
    "ax.set_zlabel('Sales')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### A word of caution on overfitting\n",
    "\n",
    "It is tempting to include a lot of terms in the regression, but this is problematic. A useful model will  *generalize* beyond the data given to it. \n",
    "\n",
    "\n",
    "**Questions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting, underfitting, model generalizability, and the bias–variance tradeoff\n",
    "\n",
    "In regression, and other prediction problems, we would like to develop a model on a dataset, that would preform well, not only on that dataset, but on similar data that the model hasn't yet seen. If a model satisfies this criterion, we say that it is *generalizable*. \n",
    "\n",
    "Consider the following data, that has been fit with a linear polynomial model (black) and a high degree polynomial model (blue). For convenience, let me call these the black and blue models, respectively. \n",
    "\n",
    "<img src=\"overfitted_data.png\" title=\"https://commons.wikimedia.org/w/index.php?curid=47471056\" width=\"40%\">\n",
    "\n",
    "Let's call the dataset that we train the model on the *training dataset* and the dataset that we test the model on the *testing dataset*. In the above figure, the training dataset are the black points and the testing dataset is not shown, but we imagine it to be similar to the points shown. \n",
    "\n",
    "Which model is better? \n",
    "\n",
    "The blue model has 100% accuracy on the training dataset, while the black model has much smaller accuracy. However, the blue model is highly oscillatory and might not generalize well to new data. For example, the model would wildly miss the test point $(3,0)$. We say that the blue model has *overfit* the data. On the other hand, it isn't difficult to see that we could also *underfit* the data. In this case, the model isn't complex enough to have good accuracy on the training dataset. \n",
    "\n",
    "This phenomena is often described in terms of the *bias-variance tradeoff*. Here, we decompose the error of the model into three terms:\n",
    "$$\n",
    "\\textrm{Error} = \n",
    "\\textrm{Bias} + \n",
    "\\textrm{Variance} + \n",
    "\\textrm{Irreducible Error}. \n",
    "$$\n",
    "- The *bias* of the method is the error caused by the simplifying assumptions built into the method. \n",
    "+ The *variance* of the method is how much the model will change based on the sampled data. \n",
    "+ The *irreducible error* is error in the data itself, so no model can capture this error. \n",
    "\n",
    "There is a tradeoff between the bias and variance of a model. \n",
    "High-variance methods (e.g., the blue method) are accurate on the training set, but overfit noise in the data, so don't generalized well to new data. High-bias models (e.g., the black method) are too simple to fit the data, but are better at generalizing to new test data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalizability in practice\n",
    "\n",
    "Consider the Auto dataset, which contains 9 features (mpg, cylinders, displacement, horsepower, weight, acceleration, year, origin, name) for 397 different used cars. This dataset is available in the lectures folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "auto = pd.read_csv('Auto.csv') #load data\n",
    "print(auto.dtypes) #Horsepower should be an integer but imported as object\n",
    "\n",
    "# some of the horsepowers are '?', so we just remove them and then map the remaining strings to integers\n",
    "auto = auto[auto.horsepower != '?']\n",
    "auto['horsepower'] = auto['horsepower'].map(int)\n",
    "\n",
    "auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(auto.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's consider the relationship between mpg and horsepower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(auto['horsepower'],auto['mpg'],color='black',linewidth=1)\n",
    "plt.xlabel('horsepower'); plt.ylabel('mpg')\n",
    "plt.ylim((0,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We consider the linear model\n",
    "$$\n",
    "\\text{mpg} = \\beta_0 + \\beta_1 \\text{horsepower} + \\beta_2 \\text{horsepower}^2 + \\cdots + \\beta_m \\text{horsepower}^m\n",
    "$$\n",
    "It might seem that choosing $m$ to be large would be a good thing. After all, a high degree polynomial is more flexible than a small degree polynomial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit polynomial models\n",
    "mr1 = sm.ols(formula=\"mpg ~ horsepower\", data=auto).fit()\n",
    "par1 = dict(mr1.params)\n",
    "mr2 = sm.ols(formula=\"mpg ~ horsepower + I(horsepower ** 2.0)\", data=auto).fit()\n",
    "par2 = dict(mr2.params)\n",
    "mr3 = sm.ols(formula=\"mpg ~ horsepower + I(horsepower ** 2.0) + I(horsepower ** 3.0)\", data=auto).fit()\n",
    "par3 = dict(mr3.params)\n",
    "mr4 = sm.ols(formula=\"mpg ~ horsepower + I(horsepower ** 2.0) + I(horsepower ** 3.0) + I(horsepower ** 4.0)\", data=auto).fit()\n",
    "par4 = dict(mr4.params)\n",
    "\n",
    "# make scatterplot of data\n",
    "plt.scatter(auto['horsepower'],auto['mpg'],color='black',label=\"data\")\n",
    "\n",
    "# evaluate polynomial models on a grid\n",
    "x = np.linspace(0,250,1000)\n",
    "y1 = par1[\"Intercept\"] + par1['horsepower']*x\n",
    "y2 = par2[\"Intercept\"] + par2['horsepower']*x + par2['I(horsepower ** 2.0)']*x**2\n",
    "y3 = par3[\"Intercept\"] + par3['horsepower']*x + par3['I(horsepower ** 2.0)']*x**2 + par3['I(horsepower ** 3.0)']*x**3\n",
    "y4 = par4[\"Intercept\"] + par4['horsepower']*x + par4['I(horsepower ** 2.0)']*x**2 + par4['I(horsepower ** 3.0)']*x**3 + par4['I(horsepower ** 4.0)']*x**4\n",
    "\n",
    "# plot polynomial models\n",
    "plt.plot(x,y1,label=\"degree 1\",linewidth=2)\n",
    "plt.plot(x,y2,label=\"degree 2\",linewidth=2)\n",
    "plt.plot(x,y3,label=\"degree 3\",linewidth=2)\n",
    "plt.plot(x,y4,label=\"degree 4\",linewidth=2)\n",
    "plt.legend()\n",
    "plt.xlabel('horsepower'); plt.ylabel('mpg')\n",
    "plt.ylim((0,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('mr1:',mr1.rsquared)\n",
    "print('mr2:',mr2.rsquared)\n",
    "print('mr3:',mr3.rsquared)\n",
    "print('mr4:',mr4.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $m$ increases, the $R^2$ value is becoming larger. (You can prove that this is always true if you add more predictors.)\n",
    "\n",
    "Let's check the $p$-values for the coefficients for the degree 4 fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mr1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mr2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mr3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mr4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "For $m>2$, the $p$-values are very large, so we don't have a strong relationship between the variables. \n",
    "\n",
    "We could rely on *Occam's razor* to decide between models. Occam's razor can be stated: among many different models that explain the data, the simplest one should be used. Since we don't get much benefit in terms of $R^2$ values by choosing $m>2$, we should use $m=2$. \n",
    "\n",
    "But there are even better criterion for deciding between models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "There is a clever method for developing generalizable models that aren't underfit or overfit, called *cross validation*. \n",
    "\n",
    "**Cross-validation** is a general method for assessing how the results of a predictive model (regression, classification,...) will *generalize* to an independent data set. In regression, cross-validation is a method for assessing how well the regression model will predict the dependent value for points that weren't used to *train* the model. \n",
    "\n",
    "The idea of the method is simple: \n",
    "+ Split the dataset into two groups: the training dataset and the testing dataset. \n",
    "+ Train a variety of models on the training dataset. \n",
    "+ Check the accuracy of each model on the testing dataset. \n",
    "+ By comparing these accuracies, determine which model is best.\n",
    "\n",
    "In practice, you have to decide how to split the data into groups (i.e. how large the groups should be). You might also want to repeat the experiment so that the assessment doesn't depend on the way in which you split the data into groups. We'll worry about these questions in a later lecture.\n",
    "\n",
    "As the model becomes more complex ($m$ increases), the accuracy always increases for the training dataset. But, at some point, it starts to overfit the data and the accuracy decreases for the test dataset! Cross validation techniques will allow us to find the sweet-spot for the parameter $m$!\n",
    "\n",
    "Let's see this concept for the relationship between mpg and horsepower in the Auto dataset. We'll use the scikit-learn package for the cross validation analysis instead of statsmodels, because it is much easier to do cross validation there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression() # create a linear regression object\n",
    "\n",
    "# add additional columns to the dataframe for the various powers of horsepower\n",
    "for m in np.arange(2,6): \n",
    "    auto['h'+str(m)] = auto['horsepower']**m\n",
    "\n",
    "# with scikit-learn, we have to extract values from the pandas dataframe\n",
    "X = auto[['horsepower','h2','h3','h4','h5']].values\n",
    "y = auto['mpg'].values\n",
    "print(X) #note X is a numpy array\n",
    "print(type(X))\n",
    "\n",
    "plt.scatter(X[:,0], y,  color='black',label='data')\n",
    "\n",
    "# make data for plotting\n",
    "xs = np.linspace(20, 250, num=100)\n",
    "Xs = np.zeros([100,5])\n",
    "Xs[:,0] = xs\n",
    "for m in np.arange(1,5): \n",
    "    Xs[:,m] = xs**(m+1)\n",
    "    \n",
    "for m in np.arange(1,6):     \n",
    "    lr.fit(X=X[:,:m], y=y)\n",
    "    plt.plot(xs, lr.predict(X=Xs[:,:m]), linewidth=3, label = \"m = \" + str(m) )\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('horsepower'); plt.ylabel('mpg')\n",
    "plt.ylim((0,50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Cross validation using scikit-learn \n",
    "\n",
    "- In scikit-learn, you can use the [*train_test_split*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split the dataset into a training dataset and a test dataset.\n",
    "+ The *score* function returns the coefficient of determination, $R^2$, of the prediction.\n",
    "\n",
    "In the following code, I've split the data in an unusual way - taking the test set to be 90% - to illustrate the point more clearly. Typically, we might make the training set to be 70-80% of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.9, random_state=1)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "plt.scatter(X_train[:,0], y_train,  color='red',label='training data')\n",
    "plt.scatter(X_test[:,0], y_test,  color='black',label='test data')\n",
    "\n",
    "for m in np.arange(1,6):     \n",
    "    lr.fit(X=X_train[:,:m], y=y_train)\n",
    "    print('m=', m, ', train: ', lr.score(X_train[:,:m], y_train), ' test: ', lr.score(X_test[:,:m], y_test))\n",
    "    plt.plot(xs, lr.predict(X=Xs[:,:m]), linewidth=3, label = \"m = \" + str(m) )\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('horsepower'); plt.ylabel('mpg')\n",
    "plt.ylim((0,50))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We observe that as the model complexity increases, \n",
    "- the accuracy on the training data increases, but \n",
    "+ the generalizability of the model to the test set decreases. \n",
    "\n",
    "Our job as data analysts is to find a model that is sufficiently complex to describe the training data, but not so complex that it isn't generalizable to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class exercise: analysis of the credit dataset \n",
    "\n",
    "We'll use [Statsmodels](http://statsmodels.sourceforge.net/) to study a dataset related to credit cards.\n",
    "We'll use the 'Credit' dataset stored in the lectures folder. \n",
    "This dataset consists of some credit card information for 400 people. \n",
    "\n",
    "Of course, a *credit card* is a card issued to a person (\"cardholder\"), typically from a bank, that can be used as a method of payment. The card allows the cardholder to borrow money from the bank to pay for goods and services. Credit cards have a *limit*, the maximum amount you can borrow, which is determined by the bank. The limit is determined from information collected from the cardholder (income, age, ...) and especially (as we will see) the cardholders \"credit rating\".  The *credit rating* is an evaluation of the (1) ability of the cardholder to pay back the borrowed money and (2) the likelihood of the cardholder to defaulting on the borrowed money. \n",
    "\n",
    "Our focus will be on the use of regression tools to study this dataset. Ideally, we'd like to understand what factors determine *credit ratings* and *credit limits*. We can think about this either from the point of view of (1) a bank who wants to protect their investments by minimizing credit defaults or (2) a person who is trying to increase their credit rating and/or credit limit. \n",
    "\n",
    "A difficulty we'll encounter is including categorical data in regression models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from Credit.csv file\n",
    "credit = pd.read_csv('Credit.csv',index_col=0) #load data\n",
    "credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize and describe data\n",
    "\n",
    "print(credit.dtypes, '\\n') \n",
    "print(credit['Gender'].value_counts(), '\\n')\n",
    "print(credit['Student'].value_counts(), '\\n')\n",
    "print(credit['Married'].value_counts(), '\\n')\n",
    "print(credit['Ethnicity'].value_counts())\n",
    "credit.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The column names of this data are:  \n",
    "+ Income \n",
    "+ Limit  \n",
    "+ Rating  \n",
    "+ Cards\n",
    "+ Age  \n",
    "+ Education  \n",
    "+ Gender (categorial: M, F)\n",
    "+ Student (categorial: Y, N)\n",
    "+ Married (categorial: Y, N)\n",
    "+ Ethnicity (categorial: Caucasian, Asian, African American) \n",
    "+ Balance\n",
    "\n",
    "**Question:** What is wrong with the income data? How can it be fixed? \n",
    "\n",
    "The file 'Credit.csv' is a comma separated file. I assume a period was used instead of a comma to indicate thousands in income so it wouldn't get confused with the separating value? Or maybe this is a dataset from Europe? Or maybe the income is just measured in \\$1k units?  To change the income data, we can use the Pandas series 'map' function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit[\"Income\"] = credit[\"Income\"].map(lambda x: 1000*x)\n",
    "print(credit[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the covariances in the data. (This is how the variables vary together.) There are two ways to do this:\n",
    "+ Quantitatively: Compute the correlation matrix. For each pair of variables, $(x_i,y_i)$, we compute \n",
    "$$\n",
    "\\frac{\\sum_i (x_i - \\bar x) (y_i - \\bar y)}{s_x s_y}\n",
    "$$ \n",
    "where $\\bar x, \\bar y$ are sample means and $s_x, s_y$ are sample variances. \n",
    "+ Visually: Make a scatter matrix of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "credit.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# trick: semi-colon prevents unwanted output\n",
    "pd.plotting.scatter_matrix(credit, figsize=(10, 10), diagonal='hist');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Observations:**\n",
    "+ Limit and Rating are highly correlated ($99.7\\%$)  \n",
    "+ Income strongly correlates with Limit ($79\\%$) and Rating ($79\\%$)\n",
    "+ Balance correlates with Limit ($86\\%$) and Rating ($86\\%$)\n",
    "+ There are \"weird stripes\" in some of the data. Why? \n",
    "+ Categorical information doesn't appear in this plot. Why? How can I visualize the categorical variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Categorical variables: Gender, Student, Married, Ethnicity\n",
    "\n",
    "# Barplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2,figsize=(8,8))\n",
    "credit[\"Gender\"].value_counts().plot(kind='bar',ax=axes[0,0],title='Gender');\n",
    "credit[\"Student\"].value_counts().plot(kind='bar',ax=axes[1,0],title='Student');\n",
    "credit[\"Married\"].value_counts().plot(kind='bar',ax=axes[0,1],title='Married');\n",
    "credit[\"Ethnicity\"].value_counts().plot(kind='bar',ax=axes[1,1],title='Ethnicity');\n",
    "fig.suptitle('Barplots of Various Categorical Variables',fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Side-by-side boxplots for Limit by categories\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2,figsize=(8,8))\n",
    "credit.boxplot(column=['Limit'], by=['Gender'],ax=axes[0,0])\n",
    "credit.boxplot(column=['Limit'], by=['Student'],ax=axes[0,1])\n",
    "credit.boxplot(column=['Limit'], by=['Married'],ax=axes[1,0])\n",
    "credit.boxplot(column=['Limit'], by=['Ethnicity'],ax=axes[1,1])\n",
    "fig.suptitle('Boxplots of Credit Limit by Various Categorical Variables', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Incorporating categorical variables into regression models\n",
    "\n",
    "We have four categorical variables (Gender, Student, Married, Ethnicity). How can we include them in a regression model? \n",
    "\n",
    "Let's start with a categorical variable with only 2 categories: Gender (Male, Female).\n",
    "\n",
    "Idea: Create a \"dummy variable\" that turns Gender into a real value: \n",
    "$$\n",
    "\\text{Gender_num}_i = \\begin{cases} \n",
    "1 & \\text{if $i$-th person is female} \\\\\n",
    "0 & \\text{if $i$-th person is male}\n",
    "\\end{cases}. \n",
    "$$\n",
    "Then we could try to fit a model of the form\n",
    "$$\n",
    "\\text{Income} = \\beta_0 + \\beta_1 \\text{Gender_num}. \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "credit[\"Gender_num\"] = credit[\"Gender\"].map({' Male':0, 'Female':1})\n",
    "credit[\"Student_num\"] = credit[\"Student\"].map({'Yes':1, 'No':0})\n",
    "credit[\"Married_num\"] = credit[\"Married\"].map({'Yes':1, 'No':0})\n",
    "\n",
    "credit_model = sm.ols(formula=\"Income ~ Gender_num\", data=credit).fit()\n",
    "credit_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Since the $p$-value for the Gender_num coefficient is very large, there is no support for the conclusion that there is a difference in income between genders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What about a categorical variable with 3 categories? \n",
    "\n",
    "The Ethnicity variable takes three values: Caucasian, Asian, and African American. \n",
    "\n",
    "What's wrong with the following?  \n",
    "$$\n",
    "\\text{Ethnicity_num}_i = \\begin{cases} \n",
    "0 & \\text{if $i$-th person is Caucasian} \\\\\n",
    "1 & \\text{if $i$-th person is Asian} \\\\ \n",
    "2 & \\text{if $i$-th person is African American}\n",
    "\\end{cases}. \n",
    "$$\n",
    "\n",
    "\n",
    "We'll need more than one dummy variable:  \n",
    "$$\n",
    "\\text{Asian}_i = \\begin{cases} \n",
    "1 & \\text{if $i$-th person is Asian} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}. \n",
    "$$\n",
    "$$\n",
    "\\text{Caucasian}_i = \\begin{cases} \n",
    "1 & \\text{if $i$-th person is Caucasian} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}. \n",
    "$$\n",
    "The value with no dummy variable--African American--is called the *baseline*.\n",
    "\n",
    "We can use the *get_dummies* function to automatically get these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dummy = pd.get_dummies(credit['Ethnicity'])\n",
    "credit = pd.concat([credit,dummy],axis=1)\n",
    "credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://imgs.xkcd.com/comics/error_bars_2x.png)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
