{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Data Science \n",
    "# Lecture 22: Neural Networks II\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we'll continue discussing Neural Networks. \n",
    "\n",
    "Recommended Reading:\n",
    "* A. Géron, [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/) (2022), Ch. 10-13. See also the [associated github page](https://github.com/ageron/handson-ml3).\n",
    "* I. Goodfellow, Y. Bengio, and A. Courville, [Deep Learning](http://www.deeplearningbook.org/) (2016)\n",
    "*  [TensorFlow tutorials](https://www.tensorflow.org/tutorials)\n",
    "*  Y. LeCun, Y. Bengio, and G. Hinton, [Deep learning](https://www.nature.com/articles/nature14539), Nature (2015) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Neural Networks\n",
    "\n",
    "Last time, we introduced *Neural Networks* and discussed how they can be used for classification and regression.\n",
    "\n",
    "There are many different *network architectures* for Neural Networks, but our focus is on **Multi-layer Perceptrons**. Here, there is an *input layer*, typically drawn on the left hand side and an *output layer*, typically drawn on the right hand side. The middle layers are called *hidden layers*. \n",
    "\n",
    "\n",
    "<img src=\"Colored_neural_network.svg\" title=\"https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Colored_neural_network.svg\" \n",
    "width=\"300\">\n",
    "\n",
    "Given a set of features $X = x^0 = \\{x_1, x_2, ..., x_n\\}$ and a target $y$, a neural network works as follows. \n",
    "\n",
    "\n",
    "Each layer applies an affine transformation and an [activation function](https://en.wikipedia.org/wiki/Activation_function) (e.g., ReLU, hyperbolic tangent, or logistic) to the output of the previous layer: \n",
    "$$\n",
    "x^{j} = \\sigma ( W^{j} x^{j-1} + b^j ). \n",
    "$$\n",
    "At the $j$-th hidden layer, the input is represented as the composition of $j$ such mappings. An additional function, *e.g.* [softmax](https://en.wikipedia.org/wiki/Softmax_function), is applied to the output layer to give the prediction, $\\hat y$, for classification problems. \n",
    "\n",
    "<img src=\"activationFct.png\" \n",
    "title=\"see Géron, Ch. 10\" \n",
    "width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Softmax function for classificaton \n",
    "\n",
    "The *softmax function*, $s:\\mathbb{R}^K \\to (0,1)^K$ is defined by\n",
    "$$\n",
    "s(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}\n",
    "\\qquad \\qquad \\textrm{for } j=1, \\ldots, K.\n",
    "$$\n",
    "Note that each component is in the range $(0,1)$ and the values sum to 1. We interpret $s(\\mathbf{z})_j$ as the probability that $\\mathbf{z}$ is a member of class $j$. \n",
    "\n",
    "**Example:** Let's say we are classifying images of cats, dogs, and fish, so that our ouput layer is 3 dimensional. \n",
    "\\begin{align*}\n",
    "z & = \\begin{bmatrix} 20 & 10 & 6 \\end{bmatrix} \\\\\n",
    "\\text{max}(z) & = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix} \\\\\n",
    "\\text{softmax}(z) & = \\begin{bmatrix} 0.999953770678767 & 0.000045397830955 & 0.000000831490278 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "The softmax function behaves similar to the max function, but it's smooth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "Neural networks uses a loss function of the form \n",
    "$$\n",
    "Loss(\\hat{y},y,W) =  \\frac{1}{2} \\sum_{i=1}^n g(\\hat{y}_i(W),y_i) + \\frac{\\alpha}{2} \\|W\\|_2^2\n",
    "$$\n",
    "Here, \n",
    "+ $y_i$ is the label for the $i$-th example, \n",
    "+ $\\hat{y}_i(W)$ is the predicted label for the $i$-th example, \n",
    "+ $g$ is a function that measures the error, typically $L^2$ difference for regression or cross-entropy for classification, and \n",
    "+ $\\alpha$ is a regularization parameter. \n",
    "\n",
    "Starting from initial random weights, the loss function is minimized by repeatedly updating these weights. Various **optimization methods** can be used, *e.g.*, \n",
    "+ gradient descent method \n",
    "+ quasi-Newton method,\n",
    "+ stochastic gradient descent, or \n",
    "+ ADAM. \n",
    "\n",
    "There are various parameters associated with each method that must be tuned. \n",
    "\n",
    "**Back propagation** is a way of using the chain rule from calculus to compute the gradient of the $Loss$ function for optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks in scikit-learn\n",
    "\n",
    "In the previous lecture, we used Neural Network implementations in scikit-learn to do both classification and regression:\n",
    "+ [multi-layer perceptron (MLP) classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "+ [multi-layer perceptron (MLP) regressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
    "\n",
    "\n",
    "However, there are several limitations to the scikit-learn implementation: \n",
    "- no GPU support\n",
    "- limited network architectures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks with TensorFlow\n",
    "\n",
    "Today, we'll use [TensorFlow](https://github.com/tensorflow/tensorflow) to train a Neural Network. \n",
    "\n",
    "TensorFlow is an open-source library designed for large-scale machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Installing TensorFlow\n",
    "\n",
    "Instructions for installing TensorFlow are available at [the tensorflow install page](https://www.tensorflow.org/install).\n",
    "\n",
    "It is recommended that you use the command: \n",
    "```\n",
    "pip install tensorflow\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "TensorFlow represents computations by connecting op (operation) nodes into a computation graph.\n",
    "\n",
    "<img src=\"graph.png\" \n",
    "width=\"400\">\n",
    "\n",
    "A TensorFlow program previously had two components:\n",
    "+ In the *construction phase*, a computational graph is built. During this phase, no computations are performed and the variables are not yet initialized. \n",
    "+ In the *execution phase*, the graph is evaluated, typically many times. In this phase, each operation is given to a CPU or GPU, variables are initialized, and functions can be evaluted. \n",
    "+ However Tensorflow 2 implements [eager execution](https://www.tensorflow.org/guide/eager) which is easier to use and eliminates the need for the user to explicitly define a construction and execution phase. \n",
    "\n",
    "For example the following can be directly evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = tf.Variable(3)\n",
    "y = tf.Variable(4)\n",
    "f = x*x*y + y + 2\n",
    "print(f.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Autodiff\n",
    "\n",
    "TensorFlow can automatically compute the derivative of functions using [```GradientTape```](https://www.tensorflow.org/guide/autodiff). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example: function of 1 variable\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x**2\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example: function of 2 variables\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "y = tf.Variable(4.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  f = x + 2*y*y + 2\n",
    "grads = tape.gradient(f, [x,y])\n",
    "print(grads[0].numpy())\n",
    "print(grads[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This is enormously helpful since training a NN requires the derivate of the loss function with respect to the parameters (and there are a lot of parameters). This is computed using backpropagation (chain rule) and TensorFlow does this work for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimization methods in Keras\n",
    "Tensorflow also has several built-in optimization methods. Here are the ones available in Keras API:\n",
    "\n",
    "Other optimization methods in TensorFlow:\n",
    "+ [```tf.keras.optimizers.Adadelta```](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta)\n",
    "+ [```tf.keras.optimizers.Adagrad```](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad)\n",
    "+ [```tf.keras.optimizers.Adam```](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n",
    "+ [```tf.keras.optimizers.Adamax```](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax)\n",
    "+ [```tf.keras.optimizers.Ftrl```](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl)\n",
    "+ [```tf.keras.optimizers.Nadam```](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Nadam)\n",
    "+ [```tf.keras.optimizers.RMSprop```](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop)\n",
    "+ [```tf.keras.optimizers.SGD```](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD)\n",
    "\n",
    "For more information, see the [Keras webpage](https://keras.io/api/optimizers/). \n",
    "\n",
    "\n",
    "Let's see how to use the stochastic gradient descent optimizer [```tf.keras.optimizers.SGD```](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fu(x1, x2): \n",
    "    return x1 ** 2.0 - x1 * 3  + x2 ** 2\n",
    "def fu_minimzie():\n",
    "    return x1 ** 2.0 - x1 * 3  + x2 ** 2\n",
    "def reset():\n",
    "    x1 = tf.Variable(10.0) \n",
    "    x2 = tf.Variable(10.0) \n",
    "    return x1, x2\n",
    "\n",
    "x1, x2 = reset()\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "for i in range(50):\n",
    "    print ('y = {:.1f}, x1 = {:.1f}, x2 = {:.1f}'.format(fu(x1, x2).numpy(), x1.numpy(), x2.numpy()))\n",
    "    opt.minimize(fu_minimzie, var_list=[x1, x2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's try using another optimizer: [```tf.keras.optimizers.Adam```](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam). You may have to adjust the learning rate to get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fu(x1, x2): \n",
    "    return x1*x1 + 10*x2*x2\n",
    "def fu_minimzie():\n",
    "    return x1*x1 + 10*x2*x2\n",
    "def reset():\n",
    "    x1 = tf.Variable(3.0) \n",
    "    x2 = tf.Variable(2.0) \n",
    "    return x1, x2\n",
    "\n",
    "x1, x2 = reset()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "for i in range(300):\n",
    "    print ('y = {:.1f}, x1 = {:.1f}, x2 = {:.1f}'.format(fu(x1, x2).numpy(), x1.numpy(), x2.numpy()))\n",
    "    opt.minimize(fu_minimzie, var_list=[x1, x2])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Change the optimizer to SGD and increase the learning rate. What can go wrong if the learning rate is too large? Note this can occur even for a nice, convex function as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 30)\n",
    "y = np.linspace(-10, 10, 30)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = fu(X, Y)\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "ax.set_title('surface');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifying MNIST using TensorFlow's Keras API \n",
    "\n",
    "We now use TensorFlow to classify the handwritten digits in the MNIST dataset.  We'll use TensorFlow's Keras API to build a NN for the MNIST dataset. \n",
    "\n",
    "[Keras](https://keras.io/) is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. We'll use it with TensorFlow. \n",
    "\n",
    "+ Here the images are 28x28 and there are 10 classes (each corresponding to a digit). We'll choose 2 hidden layers, with 300 and 100 neurons respectively. \n",
    "\n",
    "+ With Keras we can build the NN sequentially by combining layers. We specify the type and size of each layer.\n",
    "\n",
    "+ We can start with a \"flattening\" layer to vectorize the input data.\n",
    "\n",
    "+ The loss function is cross entropy. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np   \n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose NN architechture\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(300, activation='relu'),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the loss function and optimization parameters:\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each *epoch*, the code breaks the training batch into mini-batches of size 50. Cycling through the mini-batches, it trains the NN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we didn't apply the softmax, the model output isn't constrained to be in the interval (0,1). These values are referred to as *logits* in the tensorflow documentation. We need to apply a softmax to convert to a probability distribution. For example let's look at one of the test data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logits corresponding to first test data point:\n",
    "model_output = model(x_test[:1]).numpy()\n",
    "print(model_output)\n",
    "\n",
    "# These logits can also be accessed with model.predict:\n",
    "model_prediction = model.predict(x_test[:1])\n",
    "print(model_prediction)\n",
    "\n",
    "# Apply softmax to obtain probability distribution corresponding to first test data point:\n",
    "softmax_model_output = tf.nn.softmax(model_output).numpy()\n",
    "print(softmax_model_output)\n",
    "\n",
    "# Apply argmax to obtain a hard classification rule:\n",
    "class_prediction = np.argmax(model_output)\n",
    "print(class_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data:\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "# Note: the model outputs a probability distribution, but if we want to make a confusion matrix then we can \n",
    "# use the argmax function to obtain a unique predicted class for each data point:\n",
    "\n",
    "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
    "print(confusion_matrix(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Although one could add a softmax activation to the output layer (one then needs to specify from_logits=False when defining the loss), this isn't recommended for numerical stability reasons. However here's how one would adjust the above code if you really want to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose NN architechture\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(300, activation='relu'),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='SparseCategoricalCrossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model:\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate the model on the test data:\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try adding a dropout layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose NN architechture\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(300, activation='relu'),\n",
    "  tf.keras.layers.Dense(100, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),  \n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Define the loss function and optimization parameters:\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model:\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "# Evaluate the model on the test data:\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using a pre-trained network\n",
    "\n",
    "There are many examples of pre-trained NN that can be accessed [here](https://www.tensorflow.org/api_docs/python/tf/keras/applications). \n",
    "These NN are very large, having been trained on giant computers using massive datasets. \n",
    "\n",
    "It can be very useful to initialize a NN using one of these. This is called [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning). \n",
    "\n",
    "\n",
    "We'll use a NN that was pretrained for image recognition. This NN was trained on the  [ImageNet](http://www.image-net.org/) project, which contains > 14 million images belonging to > 20,000 classes (synsets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vgg_model = tf.keras.applications.VGG16(weights='imagenet',include_top=True)\n",
    "vgg_model.summary()\n",
    "\n",
    "# Note:\n",
    "# weights='imagenet': use weights pre-trained on ImageNet\n",
    "# include_top: whether to include the 3 fully-connected layers at the top of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_path = 'images/scout1.jpeg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = vgg16.preprocess_input(x)\n",
    "\n",
    "preds = vgg_model.predict(x)\n",
    "print('Predicted:', vgg16.decode_predictions(preds, top=5)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some NN topics that we didn't discuss\n",
    "+ Recurrent neural networks (RNN) for time series\n",
    "+ How NN can be used for unsupervised learning problems and [Reinforcement learning problems](https://en.wikipedia.org/wiki/Reinforcement_learning) (how agents should act to maximize some notion of cummulative reward).\n",
    "+ Special layers in NN for image processing \n",
    "+ Using Tensorflow on a GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CPU vs. GPU\n",
    "\n",
    "[CPUs (Central processing units)](https://en.wikipedia.org/wiki/Central_processing_unit) have just a few cores. The number of processes that a CPU can do in parallel is limited. However, each cores is very fast and is good for sequential tasks. \n",
    "\n",
    "[GPUs (Graphics processing units)](https://en.wikipedia.org/wiki/Graphics_processing_unit) have thousands of cores, so can do many processes in parallel. GPU cores are typically slower and are more limited than CPU cores. However, for the right kind of computations (think matrix multiplication), GPUs are very fast. GPUs also have their own memory and caching systems, which further improves the speed of some computations, but also makes GPUs more difficult to program. (You have to use something like [CUDA](https://en.wikipedia.org/wiki/CUDA)).  \n",
    "\n",
    "TensorFlow can use GPUs to significantly speed up the training NN. See the programmer's guide [here](https://www.tensorflow.org/programmers_guide/using_gpu). "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
